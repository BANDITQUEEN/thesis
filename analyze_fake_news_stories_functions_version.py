# -*- coding: utf-8 -*-
"""Analyze_fake_news_stories_functions_version

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xsd_w4sfXSC3bDmknOKBD4JtlqY72hxi
"""

!python -m spacy download el_core_news_lg

!pip install greek_stemmer

import pandas as pd
import re
import string
from typing import List, Tuple
import numpy as np
import spacy
from PIL import Image
import matplotlib.pyplot as plt
from wordcloud import WordCloud, ImageColorGenerator
import nltk
from google.colab import drive
import requests
from io import BytesIO
from greek_stemmer import GreekStemmer
from typing import Union, List, Tuple


stemmer = GreekStemmer()
nlp = spacy.load("el_core_news_lg")

def mount_google_drive():
    drive.mount('/gdrive')

def install_packages():
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('punkt_tab')
    nltk.download('averaged_perceptron_tagger')

# Data Loading and Cleaning
def load_data(file_path: str) -> pd.DataFrame:
    return pd.read_csv(file_path, on_bad_lines="skip")

def clean_text(article: str) -> str:
    clean1 = re.sub(rf"[{string.punctuation}’—”]", "", article.lower())
    return re.sub(r'\W+', ' ', clean1)

def tokenize_text(data: pd.DataFrame) -> pd.DataFrame:
    data['tokenized_title'] = data['title'].apply(clean_text)
    data['tokenized_body'] = data['body'].astype(str).apply(clean_text)
    return data

def apply_basic_cleaning(data: pd.DataFrame) -> pd.DataFrame:
    data['num_wds'] = data['tokenized_body'].apply(lambda x: len(x.split()))
    data['num_wds_title'] = data['title'].apply(lambda x: len(x.split()))
    data['uniq_wds_body'] = data['tokenized_body'].apply(lambda x: len(set(x.split())))
    return data[data['num_wds'] > 70].dropna()

# Stopwords Handling
def update_stopwords(nlp, additional_stopwords: List[str]) -> set:
    stop_words = nlp.Defaults.stop_words
    stop_words.update(additional_stopwords)
    return stop_words

def remove_stopwords(data: pd.DataFrame, stop_words: set) -> pd.DataFrame:
    pat = r'\b(?:{})\b'.format('|'.join(stop_words))
    data['without_stopwords'] = data['tokenized_body'].str.replace(pat, '', regex=True).str.replace(r'\s+', ' ', regex=True)
    return data

#Text processing - Lemmatizing and stemming
def lemmatize(text: str) -> str:
    return " ".join([token.lemma_ for token in nlp(text)])

def remove_accents_and_capitalize(text: str) -> str:
    """Removes accents and capitalizes the text in order to apply stemming"""
    accents = {
        'ά': 'Α', 'έ': 'Ε', 'ή': 'Η', 'ί': 'Ι', 'ό': 'Ο', 'ύ': 'Υ', 'ώ': 'Ω',
        'ϊ': 'Ι', 'ϋ': 'Υ', 'ΐ': 'Ι', 'ΰ': 'Υ',
        'Ά': 'Α', 'Έ': 'Ε', 'Ή': 'Η', 'Ί': 'Ι', 'Ό': 'Ο', 'Ύ': 'Υ', 'Ώ': 'Ω'
    }
    for accented_char, unaccented_char in accents.items():
        text = text.replace(accented_char, unaccented_char)
    return text.upper()

def stemming(text: str) -> str:
    processed_text = remove_accents_and_capitalize(text)
    return " ".join([stemmer.stem(token.text) for token in nlp(processed_text)])

def apply_text_processing(data: pd.DataFrame) -> pd.DataFrame:
    """Applies lemmatization and updated stemming to title and body columns."""
    data['lemmatized_title'] = data['title'].apply(lemmatize)
    data['lemmatized_body'] = data['body'].apply(lemmatize)
    data['stemmed_title'] = data['title'].apply(stemming)
    data['stemmed_body'] = data['body'].apply(stemming)

    return data

# Sentiment Analysis with NRC and AIL Lexicons
def load_emotion_lexicons(nrc_lexicon_path: str, ail_lexicon_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
    emolex_df = pd.read_csv(nrc_lexicon_path).drop_duplicates(subset=['word']).dropna().reset_index(drop=True)
    ail_df = pd.read_csv(ail_lexicon_path)
    return emolex_df, ail_df

def process_nrc_lexicon_with_lemmatization(emolex_df: pd.DataFrame) -> pd.DataFrame:
    """Applies lemmatization to terms in the NRC lexicon to match with the lemmatized text."""
    emolex_df['term_lemmatized'] = emolex_df['word'].apply(lemmatize)
    return emolex_df

def process_ail_lexicon_with_stemming(ail_df: pd.DataFrame) -> pd.DataFrame:
    ail_df['term'] = ail_df['term'].apply(stemming)
    ail_df = ail_df.pivot_table(index='term', columns='AffectDimension', values='score', fill_value=0).reset_index()
    ail_df.columns.name = None
    return ail_df

# Calculate Emotion Scores Using Lemmatized NRC Lexicon
#First, NRC Lexicon with lemmatization
def calculate_emotion_scores(data: pd.DataFrame, emolex_df: pd.DataFrame) -> pd.DataFrame:
    """Calculates emotion scores based on lemmatized NRC lexicon terms."""

    w2emotions = {row['term_lemmatized']: {emotion: row[emotion] for emotion in ['Anger', 'Positive', 'Joy', 'Disgust', 'Surprise', 'Trust', 'Anticipation', 'Sadness', 'Negative', 'Fear']} for idx, row in emolex_df.iterrows()}

    def get_emotion_score(doc, emotion):
        """Calculates score for a specified emotion using lemmatized words in the document."""
        tokens = doc.split()
        return sum([w2emotions.get(word, {}).get(emotion, 0) for word in tokens if word in w2emotions])

    # Calculate emotion scores for each emotion using lemmatized text
    emotions = ['Anger', 'Positive', 'Joy', 'Disgust', 'Surprise', 'Trust', 'Anticipation', 'Sadness', 'Negative', 'Fear']
    for emotion in emotions:
        data[emotion.lower()] = data['lemmatized_body'].apply(lambda x: get_emotion_score(x, emotion))

    return data

#AIL Lexicon with stemming
def calculate_intensity_scores(data: pd.DataFrame, ail_df: Union[pd.DataFrame, dict]) -> pd.DataFrame:
    if isinstance(ail_df, dict):
        w2affects = ail_df
    else:
        w2affects = {
            row['term']: {col: row[col] for col in ail_df.columns if col != 'term'}
            for _, row in ail_df.iterrows()
        }

    def get_emotion_score(doc, emotion, tokenizer=nltk.word_tokenize, agg='mean'):
        tokens = tokenizer(doc) if isinstance(doc, str) else doc
        matches = [w2affects[w][emotion] for w in tokens if w in w2affects and emotion in w2affects[w]]
        if not matches:
            return 0
        scores = pd.DataFrame(matches)
        if agg == 'mean':
            return scores.mean().values[0]
        elif agg == 'max':
            return scores.max().values[0]
        else:
            return scores.sum().values[0]

    emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']
    for emotion in emotions:
        data[f'{emotion}_intensity'] = data['stemmed_body'].apply(lambda x: get_emotion_score(x, emotion))

    return data


# POS Tagging
def pos_analysis(data: pd.DataFrame, nlp) -> pd.DataFrame:
    pos_tags = []
    for doc in nlp.pipe(data['tokenized_body'].astype('unicode').values, batch_size=50):
        pos_tags.append([token.pos_ for token in doc])
    data['pos_tags'] = pos_tags
    data['nouns'] = data['pos_tags'].apply(lambda x: x.count("NOUN"))
    data['verbs'] = data['pos_tags'].apply(lambda x: x.count("VERB"))
    data['adjectives'] = data['pos_tags'].apply(lambda x: x.count("ADJ"))
    data['adverbs'] = data['pos_tags'].apply(lambda x: x.count("ADV"))
    data['pronouns'] = data['pos_tags'].apply(lambda x: x.count("PRON"))
    data['punctuation'] = data['pos_tags'].apply(lambda x: x.count("PUNCT"))
    data['noun_verb_ratio'] = data['nouns'] / data['verbs']
    return data

def calculate_comparative_superlative_counts(data: pd.DataFrame) -> pd.DataFrame:
    comparative_pattern = r"(υτερος|υτερη|υτερο|οτερος|οτερη|οτερο|εστερος|εστερη|εστερο|υτεροι|υτερες|υτερα|εστεροι|εστερες|εστερα|οτεροι|οτερες|οτατα)$"
    superlative_pattern = r"(οτατος|οτατη|οτατο|υτατος|υτατη|υτατο|εστατος|εστατη|εστατο|οτατοι|οτατα|οτατες|υτατοι|υτατες|υτατα|εστατοι|εστατες|εστατα)$"
    data['comparative_count'] = data['tokenized_body'].apply(lambda text: len(re.findall(comparative_pattern, text)))
    data['superlative_count'] = data['tokenized_body'].apply(lambda text: len(re.findall(superlative_pattern, text)))
    return data

# Profanity Detection
def load_profanity_list(profanity_path: str) -> List[str]:
    profanities = np.loadtxt(profanity_path, delimiter=',', skiprows=1, dtype=str).tolist()
    return [word.strip() for word in profanities]

def count_profanities(data: pd.DataFrame, profanities: List[str]) -> pd.DataFrame:
    profanity_pattern = r'\b(?:{})\b'.format('|'.join(profanities))
    data['profanity_count'] = data['tokenized_body'].apply(lambda text: len(re.findall(profanity_pattern, text)))
    return data

# Visualization and Output
def generate_word_cloud(text: str, stopwords: set, mask_image_url: str):
    response = requests.get(mask_image_url)
    mask = np.array(Image.open(BytesIO(response.content)))
    wc = WordCloud(stopwords=stopwords, background_color="black", max_words=1000, mask=mask, max_font_size=90, random_state=42)
    wc.generate(text)
    plt.figure(figsize=[15,15])
    plt.imshow(wc.recolor(color_func=ImageColorGenerator(mask)), interpolation="bilinear")
    plt.axis("off")
    plt.show()

def save_data(data: pd.DataFrame, file_name: str):
    data.to_csv(file_name, index=False)

# Main Processing Pipeline
def process_data_pipeline(file_path: str, nrc_lexicon_path: str, ail_lexicon_path: str, profanity_path: str, mask_image_url: str) -> pd.DataFrame:

    # Mount Google Drive and install packages if needed
    mount_google_drive()
    install_packages()

    # Load data
    data = load_data(file_path)
    data = tokenize_text(data)
    data = apply_basic_cleaning(data)

    emolex_df, ail_df = load_emotion_lexicons(nrc_lexicon_path, ail_lexicon_path)
    print("\nColumns in AIL lexicon (ail_df):")
    print(ail_df.columns.tolist())
    print("\nSample of AIL lexicon data:")
    print(ail_df.head())


    # Process lexicons
    emolex_df = process_nrc_lexicon_with_lemmatization(emolex_df)  # Lemmatize NRC lexicon
    ail_df = process_ail_lexicon_with_stemming(ail_df)  # Stem AIL lexicon
    print("\nSample of AIL lexicon with stemming applied:")
    print(ail_df[['term', 'term', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']].head())

    # Apply lemmatization and stemming to text data
    data = apply_text_processing(data)

    # Calculate emotion scores using lemmatized NRC lexicon
    data = calculate_emotion_scores(data, emolex_df)

    # Calculate intensity scores using stemmed AIL lexicon
    data = calculate_intensity_scores(data, ail_df)

    # POS tagging
    data = pos_analysis(data, nlp)
    data = calculate_comparative_superlative_counts(data)

    # Profanity detection
    profanities = load_profanity_list(profanity_path)
    data = count_profanities(data, profanities)

    # Stopwords removal and word cloud visualization
    additional_stopwords = ['δήλωσε', 'κάνει', 'είπε', 'κάνουν', 'βρίσκονται', 'δύο', 'υπάρχει', 'λέει', 'σύμφωνα']
    stopwords = update_stopwords(nlp, additional_stopwords)
    data = remove_stopwords(data, stopwords)

    text_body = " ".join(data['tokenized_body'])
    generate_word_cloud(text_body, stopwords, mask_image_url)

    # Save processed data
    save_data(data, "analyze_fake_news_stories.csv")

    return data

# Define file paths
data_file_path = "/gdrive/My Drive/Colab Notebooks/hoaxes2.csv"
nrc_lexicon_path = "https://raw.githubusercontent.com/datajour-gr/Data_journalism/master/week10/NRC_GREEK_Translated_6_2020.csv"
ail_lexicon_path = "/gdrive/My Drive/Colab Notebooks/greek_intensity.csv"
profanity_path = "/gdrive/My Drive/Colab Notebooks/ivristiko_lex.txt"
mask_image_url = "https://www.pinclipart.com/picdir/middle/522-5225314_transparent-detective-pikachu-png-clipart.png"

processed_data = process_data_pipeline(
    file_path=data_file_path,
    nrc_lexicon_path=nrc_lexicon_path,
    ail_lexicon_path=ail_lexicon_path,
    profanity_path=profanity_path,
    mask_image_url=mask_image_url
)

pd.set_option('display.max_columns', None)
processed_data.head()